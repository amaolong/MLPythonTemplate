{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skrf  has  12  parameter combinations\n",
      "skrf 's actual param comb:  12\n",
      "skef  has  12  parameter combinations\n",
      "skef 's actual param comb:  12\n",
      "sknn  has  48  parameter combinations\n",
      "sknn 's actual param comb:  48\n",
      "sksvc  has  12  parameter combinations\n",
      "sksvc 's actual param comb:  12\n",
      "xgb  has  12  parameter combinations\n",
      "xgb 's actual param comb:  12\n",
      "lgbm  has  24  parameter combinations\n",
      "lgbm 's actual param comb:  24\n",
      "skrf 12\n",
      "skef 12\n",
      "sknn 48\n",
      "sksvc 12\n",
      "xgb 12\n",
      "lgbm 24\n",
      "total models sampled:  18\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('C:\\\\Users\\\\yynst\\\\PycharmProjects\\\\MLPythonTemplate')\n",
    "# customized class and functions\n",
    "from ML.stacklearn.param_handling import *\n",
    "from ML.stacklearn.classification.sklearn_params import *\n",
    "from ML.stacklearn.classification.xgb_params_sklearn_interface import *\n",
    "from ML.stacklearn.stack_model import *\n",
    "\n",
    "#\n",
    "# scikit-learn\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "from sklearn.ensemble import ExtraTreesClassifier as EFC\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "# xgboost\n",
    "import xgboost as xgb\n",
    "# lgbm\n",
    "import lightgbm as lgbm\n",
    "# general import\n",
    "import numpy as np\n",
    "#\n",
    "model_dict={}\n",
    "model_dict['sksvc']=SVC\n",
    "model_dict['skrf']=RFC\n",
    "model_dict['skef']=EFC\n",
    "model_dict['sknn']=KNN\n",
    "model_dict['xgb']=xgb.XGBClassifier\n",
    "model_dict['lgbm']=lgbm.LGBMClassifier\n",
    "\n",
    "''' FLAGS '''\n",
    "model_level=2    # 2 or 3\n",
    "debug=True\n",
    "\n",
    "''' handling model parameters '''\n",
    "# pull model type, populate different parameters and sample a few to be used as base learners\n",
    "\n",
    "models=populate_params(param_collection_sk_default,param_collection_names_sk_default,0)\n",
    "for _ in populate_params(param_collection_xgb_lgbm_default,param_collection_names_xgb_lgbm_default,0):\n",
    "    models.append(_)\n",
    "\n",
    "for _ in models:\n",
    "    _.sample()\n",
    "\n",
    "model_count=0\n",
    "for _ in models:\n",
    "    for _2 in _.sampled_model_params:\n",
    "        model_count+=1\n",
    "print('total models sampled: ',model_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''insert model params'''\n",
    "level_1_models=[]   # a list\n",
    "level_2_models=[]   # 1 or a list\n",
    "level_3_models=[]   # optional\n",
    "\n",
    "for _ in models:\n",
    "    for _2 in _.sampled_model_params:\n",
    "        a=model_dict[_.model_name](**_2)\n",
    "        level_1_models.append(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'base_score': 0.5, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'subsample': 1, 'objective': 'binary:logistic', 'nthread': 5, 'gamma': 0, 'reg_alpha': 0.4, 'reg_lambda': 0.8, 'colsample_bytree': 1, 'colsample_bylevel': 1}, {'base_score': 0.5, 'learning_rate': 0.1, 'max_depth': 10, 'n_estimators': 100, 'subsample': 0.8, 'objective': 'binary:logistic', 'nthread': 5, 'gamma': 0, 'reg_alpha': 0.4, 'reg_lambda': 0.8, 'colsample_bytree': 1, 'colsample_bylevel': 1}, {'base_score': 0.5, 'learning_rate': 0.1, 'max_depth': 10, 'n_estimators': 50, 'subsample': 0.8, 'objective': 'binary:logistic', 'nthread': 5, 'gamma': 0, 'reg_alpha': 0.4, 'reg_lambda': 0.8, 'colsample_bytree': 1, 'colsample_bylevel': 1}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[{'base_score': 0.5,\n",
       "   'colsample_bylevel': 1,\n",
       "   'colsample_bytree': 1,\n",
       "   'gamma': 0,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 100,\n",
       "   'nthread': 5,\n",
       "   'objective': 'binary:logistic',\n",
       "   'reg_alpha': 0.4,\n",
       "   'reg_lambda': 0.8,\n",
       "   'subsample': 1},\n",
       "  {'base_score': 0.5,\n",
       "   'colsample_bylevel': 1,\n",
       "   'colsample_bytree': 1,\n",
       "   'gamma': 0,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 100,\n",
       "   'nthread': 5,\n",
       "   'objective': 'binary:logistic',\n",
       "   'reg_alpha': 0.4,\n",
       "   'reg_lambda': 0.8,\n",
       "   'subsample': 0.8},\n",
       "  {'base_score': 0.5,\n",
       "   'colsample_bylevel': 1,\n",
       "   'colsample_bytree': 1,\n",
       "   'gamma': 0,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 50,\n",
       "   'nthread': 5,\n",
       "   'objective': 'binary:logistic',\n",
       "   'reg_alpha': 0.4,\n",
       "   'reg_lambda': 0.8,\n",
       "   'subsample': 0.8}]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_params=[]\n",
    "for _ in models:\n",
    "    if (_=='xgb'):\n",
    "        xgb_params.append(_.sampled_model_params)\n",
    "        print(_.sampled_model_params)\n",
    "        \n",
    "xgb_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_score': 0.5,\n",
       " 'colsample_bylevel': 1,\n",
       " 'colsample_bytree': 1,\n",
       " 'gamma': 0,\n",
       " 'learning_rate': 0.1,\n",
       " 'max_depth': 10,\n",
       " 'n_estimators': 100,\n",
       " 'nthread': 5,\n",
       " 'objective': 'binary:logistic',\n",
       " 'reg_alpha': 0.4,\n",
       " 'reg_lambda': 0.8,\n",
       " 'subsample': 0.8}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_params=np.array(xgb_params)\n",
    "xgb_params[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# level_2_models\n",
    "level_2_models.append(model_dict['xgb'](**xgb_params[0,1]))\n",
    "\n",
    "\n",
    "# tune sample weights \n",
    "\n",
    "\n",
    "# try out fit and predict with random data and fixed structure \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# try customized structure"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
