{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skrf  has  12  parameter combinations\n",
      "skrf 's actual param comb:  12\n",
      "skef  has  12  parameter combinations\n",
      "skef 's actual param comb:  12\n",
      "sknn  has  48  parameter combinations\n",
      "sknn 's actual param comb:  48\n",
      "sksvc  has  12  parameter combinations\n",
      "sksvc 's actual param comb:  12\n",
      "xgb  has  12  parameter combinations\n",
      "xgb 's actual param comb:  12\n",
      "lgbm  has  24  parameter combinations\n",
      "lgbm 's actual param comb:  24\n",
      "skrf 12\n",
      "skef 12\n",
      "sknn 48\n",
      "sksvc 12\n",
      "xgb 12\n",
      "lgbm 24\n",
      "total models sampled:  18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\yynst\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('C:\\\\Users\\\\yynst\\\\PycharmProjects\\\\MLPythonTemplate')\n",
    "# customized class and functions\n",
    "from ML.stacklearn.param_handling import *\n",
    "from ML.stacklearn.classification.sklearn_params import *\n",
    "from ML.stacklearn.classification.xgb_params_sklearn_interface import *\n",
    "from ML.stacklearn.stack_model import *\n",
    "\n",
    "#\n",
    "# scikit-learn\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "from sklearn.ensemble import ExtraTreesClassifier as EFC\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "# xgboost\n",
    "import xgboost as xgb\n",
    "# lgbm\n",
    "import lightgbm as lgbm\n",
    "# general import\n",
    "import numpy as np\n",
    "#\n",
    "model_dict={}\n",
    "model_dict['sksvc']=SVC\n",
    "model_dict['skrf']=RFC\n",
    "model_dict['skef']=EFC\n",
    "model_dict['sknn']=KNN\n",
    "model_dict['xgb']=xgb.XGBClassifier\n",
    "model_dict['lgbm']=lgbm.LGBMClassifier\n",
    "\n",
    "''' FLAGS '''\n",
    "model_level=2    # 2 or 3\n",
    "debug=True\n",
    "\n",
    "''' handling model parameters '''\n",
    "# pull model type, populate different parameters and sample a few to be used as base learners\n",
    "\n",
    "models=populate_params(param_collection_sk_default,param_collection_names_sk_default,0)\n",
    "for _ in populate_params(param_collection_xgb_lgbm_default,param_collection_names_xgb_lgbm_default,0):\n",
    "    models.append(_)\n",
    "\n",
    "for _ in models:\n",
    "    _.sample()\n",
    "\n",
    "model_count=0\n",
    "for _ in models:\n",
    "    for _2 in _.sampled_model_params:\n",
    "        model_count+=1\n",
    "print('total models sampled: ',model_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''insert model params'''\n",
    "level_1_models=[]   # a list\n",
    "level_2_models=[]   # 1 or a list\n",
    "level_3_models=[]   # optional\n",
    "\n",
    "for _ in models:\n",
    "    for _2 in _.sampled_model_params:\n",
    "        a=model_dict[_.model_name](**_2)\n",
    "        level_1_models.append(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'objective': 'binary:logistic', 'colsample_bytree': 1, 'colsample_bylevel': 1, 'learning_rate': 0.1, 'n_estimators': 50, 'gamma': 0, 'subsample': 1, 'reg_lambda': 0.8, 'max_depth': 10, 'reg_alpha': 0.4, 'nthread': 5, 'base_score': 0.5}, {'objective': 'binary:logistic', 'colsample_bytree': 1, 'colsample_bylevel': 1, 'learning_rate': 0.1, 'n_estimators': 100, 'gamma': 0, 'subsample': 0.8, 'reg_lambda': 0.8, 'max_depth': 10, 'reg_alpha': 0.4, 'nthread': 5, 'base_score': 0.5}, {'objective': 'binary:logistic', 'colsample_bytree': 1, 'colsample_bylevel': 1, 'learning_rate': 0.1, 'n_estimators': 100, 'gamma': 0, 'subsample': 1, 'reg_lambda': 0.8, 'max_depth': 10, 'reg_alpha': 0.4, 'nthread': 5, 'base_score': 0.5}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[{'base_score': 0.5,\n",
       "   'colsample_bylevel': 1,\n",
       "   'colsample_bytree': 1,\n",
       "   'gamma': 0,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 50,\n",
       "   'nthread': 5,\n",
       "   'objective': 'binary:logistic',\n",
       "   'reg_alpha': 0.4,\n",
       "   'reg_lambda': 0.8,\n",
       "   'subsample': 1},\n",
       "  {'base_score': 0.5,\n",
       "   'colsample_bylevel': 1,\n",
       "   'colsample_bytree': 1,\n",
       "   'gamma': 0,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 100,\n",
       "   'nthread': 5,\n",
       "   'objective': 'binary:logistic',\n",
       "   'reg_alpha': 0.4,\n",
       "   'reg_lambda': 0.8,\n",
       "   'subsample': 0.8},\n",
       "  {'base_score': 0.5,\n",
       "   'colsample_bylevel': 1,\n",
       "   'colsample_bytree': 1,\n",
       "   'gamma': 0,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 100,\n",
       "   'nthread': 5,\n",
       "   'objective': 'binary:logistic',\n",
       "   'reg_alpha': 0.4,\n",
       "   'reg_lambda': 0.8,\n",
       "   'subsample': 1}]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_params=[]\n",
    "for _ in models:\n",
    "    if (_=='xgb'):\n",
    "        xgb_params.append(_.sampled_model_params)\n",
    "        print(_.sampled_model_params)\n",
    "        \n",
    "xgb_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=10,\n",
      "       min_child_weight=1, missing=None, n_estimators=100, nthread=5,\n",
      "       objective='binary:logistic', reg_alpha=0.4, reg_lambda=0.8,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.8)]\n"
     ]
    }
   ],
   "source": [
    "xgb_params=np.array(xgb_params)\n",
    "xgb_params[0,1]\n",
    "# level_2_models\n",
    "level_2_models.append(model_dict['xgb'](**xgb_params[0,1]))\n",
    "print(level_2_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 30)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import *\n",
    "data=load_breast_cancer()\n",
    "\n",
    "X=data.data[:300]\n",
    "y=data.target[:300]\n",
    "X1=data.data[300:]\n",
    "y1=data.target[300:]\n",
    "\n",
    "print(np.shape(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "level_1_models[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of level 1 models:  18\n",
      "number of level 2 models:  1\n"
     ]
    }
   ],
   "source": [
    "# # pseudo feature and target\n",
    "# X=np.random.random([50,10])   # 50 obs each with 10 features\n",
    "# y=np.random.randint(0,2,50)    # 50 labels\n",
    "\n",
    "# X1=np.random.random([50,10])   # 50 obs each with 10 features\n",
    "# y1=np.random.randint(0,2,50)    # 50 labels\n",
    "\n",
    "del clf,clf2\n",
    "\n",
    "# try out fit and predict with random data and fixed structure \n",
    "clf=stack_model(X,y,level_1_models,level_2_models)\n",
    "clf2=stack_model(X,y,level_1_models,level_2_models)\n",
    "\n",
    "\n",
    "# try customized structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting level 1 model -  0  out of  18\n",
      "fitting level 1 model -  1  out of  18\n",
      "fitting level 1 model -  2  out of  18\n",
      "fitting level 1 model -  3  out of  18\n",
      "fitting level 1 model -  4  out of  18\n",
      "fitting level 1 model -  5  out of  18\n",
      "fitting level 1 model -  6  out of  18\n",
      "fitting level 1 model -  7  out of  18\n",
      "fitting level 1 model -  8  out of  18\n",
      "fitting level 1 model -  9  out of  18\n",
      "fitting level 1 model -  10  out of  18\n",
      "fitting level 1 model -  11  out of  18\n",
      "fitting level 1 model -  12  out of  18\n",
      "fitting level 1 model -  13  out of  18\n",
      "fitting level 1 model -  14  out of  18\n",
      "fitting level 1 model -  15  out of  18\n",
      "fitting level 1 model -  16  out of  18\n",
      "fitting level 1 model -  17  out of  18\n",
      "fitting level 2 model -  0  out of  1\n"
     ]
    }
   ],
   "source": [
    "clf.fit2()\n",
    "output1=clf.predict2(X1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting level 1 model -  0  out of  18\n",
      "fitting level 1 model -  1  out of  18\n",
      "fitting level 1 model -  2  out of  18\n",
      "fitting level 1 model -  3  out of  18\n",
      "fitting level 1 model -  4  out of  18\n",
      "fitting level 1 model -  5  out of  18\n",
      "fitting level 1 model -  6  out of  18\n",
      "fitting level 1 model -  7  out of  18\n",
      "fitting level 1 model -  8  out of  18\n",
      "fitting level 1 model -  9  out of  18\n",
      "fitting level 1 model -  10  out of  18\n",
      "fitting level 1 model -  11  out of  18\n",
      "fitting level 1 model -  12  out of  18\n",
      "fitting level 1 model -  13  out of  18\n",
      "fitting level 1 model -  14  out of  18\n",
      "fitting level 1 model -  15  out of  18\n",
      "fitting level 1 model -  16  out of  18\n",
      "fitting level 1 model -  17  out of  18\n",
      "fitting level 2 model -  0  out of  1\n"
     ]
    }
   ],
   "source": [
    "clf.fit2_w_raw_features()\n",
    "output2=clf.predict_2_w_raw_features(X1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "12\n",
      "0.0520446096654\n",
      "0.0446096654275\n",
      "XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=25,\n",
      "       min_child_weight=1, missing=None, n_estimators=1000, nthread=5,\n",
      "       objective='binary:logistic', reg_alpha=0.4, reg_lambda=0.8,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.8)\n",
      "19.0555555556   8   67 269 203\n",
      "[ 0.00453309  0.0643699   0.00362647  0.00634633  0.0788758   0.00362647\n",
      "  0.00181324  0.04805077  0.06708976  0.00543971  0.0199456   0.00453309\n",
      "  0.03082502  0.05711696  0.00725295  0.0888486   0.00725295  0.0299184\n",
      "  0.07162285  0.01722575  0.0099728   0.08250227  0.01903898  0.07978241\n",
      "  0.05077063  0.00634633  0.03898459  0.05983681  0.0299184   0.00453309]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'argmmax'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-befa8e4a3a7d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mbest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_vec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0mworst\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_vec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlevel_1_models\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbest\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'argmmax'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "print(np.sum(np.abs(y1-output1)))\n",
    "print(np.sum(np.abs(y1-output2)))\n",
    "print(np.sum(np.abs(y1-output1))/len(y1))\n",
    "print(np.sum(np.abs(y1-output2))/len(y1))\n",
    "\n",
    "clf_ref=xgb.XGBClassifier(**xgb_params[0,1])\n",
    "tmp_dict={}\n",
    "tmp_dict['n_estimators']=1000\n",
    "tmp_dict['max_depth']=25\n",
    "print(clf_ref.set_params(**tmp_dict))\n",
    "clf_ref.fit(X,y)\n",
    "output_ref=clf_ref.predict(X1)\n",
    "np.sum(np.abs(y1-output_ref))\n",
    "\n",
    "output_vec=[]\n",
    "for _ in level_1_models:\n",
    "    _.fit(X,y)\n",
    "    output_vec.append(np.sum(np.abs(y1-_.predict(X1))))\n",
    "\n",
    "print(np.mean(output_vec),' ',np.min(output_vec),' ',np.max(output_vec) , len(y1), np.sum(y1))\n",
    "print(clf_ref.feature_importances_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBMClassifier(boosting_type='gbdt', colsample_bytree=1, learning_rate=0.1,\n",
      "        max_bin=255, max_depth=10, min_child_samples=20,\n",
      "        min_child_weight=0.001, min_split_gain=0, n_estimators=100,\n",
      "        nthread=5, num_leaves=31, objective='binary', reg_alpha=0.4,\n",
      "        reg_lambda=0.8, seed=0, silent=True, subsample=1,\n",
      "        subsample_for_bin=50000, subsample_freq=1)\n",
      "SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n"
     ]
    }
   ],
   "source": [
    "best=np.argmin(output_vec)\n",
    "worst=np.argmax(output_vec)\n",
    "\n",
    "print(level_1_models[best])\n",
    "print(level_1_models[worst])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
